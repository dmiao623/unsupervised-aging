"""Compare multiple Keypoint-MoSeq models via expected marginal likelihood.

Computes expected marginal likelihood (EML) scores across several trained
KPMS models and writes the results to a JSON file. See the `Keypoint-MoSeq
docs <https://keypoint-moseq.readthedocs.io/en/latest/advanced.html
#selecting-a-model>`_ for guidance on interpreting and plotting EML scores.

SLURM Template:
    scripts/templates/kpms_model_comparison.sh

Usage::

    python kpms_model_comparison.py \\
        --project_name <project_name> \\
        --model_basename <model_basename> \\
        --kpms_dir <path_to_kpms_projects> \\
        --num_models <num_models> \\
        --result_path <result_path>
"""

import argparse
import json
import pandas as pd

from pathlib import Path

import keypoint_moseq as kpms


def main(
    project_name: str,
    model_basename: str,
    kpms_dir: str,
    num_models: int,
    result_path: str,
):
    """Compute and save EML scores for a set of trained KPMS models.

    Model names are generated by appending ``1`` through *num_models* to
    *model_basename* (e.g. ``"model-"`` produces ``"model-1"``,
    ``"model-2"``, etc.). The resulting scores and standard errors are
    printed as a table and written to *result_path* as JSON.

    Args:
        project_name: Name of the KPMS project (subdirectory of *kpms_dir*).
        model_basename: Prefix used to generate model names.
        kpms_dir: Parent directory containing KPMS project directories.
        num_models: Number of trained models to compare.
        result_path: File path where the JSON results will be written.
    """
    kpms_dir = Path(kpms_dir)
    project_dir = kpms_dir / project_name
    result_path = Path(result_path)

    model_names = [f"{model_basename}{i}" for i in range(1, num_models + 1)]
    eml_scores, eml_std_errs = kpms.expected_marginal_likelihoods(
        project_dir, model_names
    )

    data = {
        "models": model_names,
        "eml_scores": eml_scores.tolist(),
        "eml_std_errs": eml_std_errs.tolist(),
    }
    print(pd.DataFrame(data))

    result_path.parent.mkdir(parents=True, exist_ok=True)
    with result_path.open("w") as f:
        json.dump(data, f, indent=2)
    print(f"comparison of {num_models} saved to {result_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Compare KPMS models via expected marginal likelihood.",
    )

    parser.add_argument(
        "--project_name",
        type=str,
        required=True,
        help="Name of the keypoint-MoSeq project",
    )
    parser.add_argument(
        "--model_basename",
        type=str,
        required=True,
        help="Basename of keypoint-MoSeq model (e.g. model- generates model-1, model-2, ...)",
    )
    parser.add_argument(
        "--kpms_dir",
        type=str,
        required=True,
        help="Path of the keypoint-MoSeq project directory",
    )
    parser.add_argument(
        "--num_models", type=int, required=True, help="Number of keypoint-MoSeq models"
    )
    parser.add_argument(
        "--result_path",
        type=str,
        required=True,
        help="Path to write the result JSON to",
    )

    args = parser.parse_args()

    print("\n--- RUN CONFIG ---")
    for k, v in vars(args).items():
        print(f"{k:20}: {v}")
    print("------------------\n")

    main(
        args.project_name,
        args.model_basename,
        args.kpms_dir,
        args.num_models,
        args.result_path,
    )
